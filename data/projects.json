{
  "projects": [
    {
      "id": "single-neuron-model",
      "title": "Single Neuron Model for Classification and Regression",
      "description": "In this project a single neuron model is implemented that can be used for both classification and regression tasks",
      "image": "assets\\projects\\single-neuron-model\\single-neuron-model.gif",
      "readTime": "15 minute read",
      "tags": ["Perceptron", "Classification","Regression", "Gradient Descent", "Optimization perspective"],
      "sections": [
        {
          "id": "project-overview",
          "title": "Project Overview",
          "content": "<p>This project focuses on building a single neuron model that uses gradient descent with back propogation. A linear activation function is used for Regression and a Sigmoid function is used for Classification</p>",
          "subsections": [
            {
              "id": "context",
              "title": "Context",
              "content": "<p>The history of Artificial Intelligence traces way back to 1943 on a paper called <a href=\"https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf\" target=\"_blank\">A Logical Calculus of the Ideas Immanent in Nervous Activity</a>. From a single Neuron -> Single layer network -> Multi Layer Perceptron, the development in the space of machine learning has come a long way. This project aims at developing the fundamental unit or the basic building block in artificial neural network called the neuron.</p>"
            },
            {
              "id": "actions",
              "title": "Actions",
              "content": "<p>The implementation of the single neuron model is as follows:</p><ul><li>A base class for single neuron model that implements the forward function and initialize weights</li><li>Single Neuron regression Model sub class that implements the activation and gradient function</li><li>A single neuron classifcation model sub class that implements the activation and gradient function</li></ul>"
            },
            {
              "id": "results",
              "title": "Results",
              "content": "<p>An agnostic single neuron model that can be used for any classification and regression model. This shows the power of a single neuron which when implemented in deep layers can produce astonishing results</p>"
            },
            {
              "id": "growth",
              "title": "Growth/Next Steps",
              "content": "<p>Although this model is for educational purposes,</p><ul><li>This model can be extended to a single layer neural network with multiple neurons</li><li>The single layer can further be extended to produce a multi layer perceptron capable of producing impressive results by creating complex decision boundaries.</li></ul>"
            }
          ]
        },
        {
          "id": "data-overview",
          "title": "Sample Data Overview",
          "content": "<p>The model can be used for both classification and regression tasks.</p><ul><li>For the classification task, wine quality dataset is used</li><li>For the regression task, advertising dataset is used</li></ul>"
        },
        {
          "id": "algorithm-overview",
          "title": "Single Neuron Model for Classification and Regression",
          "content": "<p>Our single neuron models consist of a single neuron or node, that sums weighted multiplications of the features of an input sample, adds a bias term, and then passes that sum through some activation function. For regression, we will use a linear activation function, i.e., just the identity function. For classification, we will use a sigmoid function.</p><p>More specifically, our single neuron model will take the dot product of an input training example $x^{(i)}$ with some learned weights $w$ and adding a learned bias $w_0$ to produce a pre-activation $z$. We then apply some activation function $f$ to $z$ to produce an activation $a$, which for this single neuron will be our output prediction $\\hat{y}$. Importantly, $x$ and $w$ can be vectors, which we will now represent as NumPy arrays in our Python code. We will use a subscript notation, e.g., $x_j$ to indicate feature $j$ within data input $x$, where $j$ goes from 1 to $m$ total features.</p><p>The following is the formal mathematical notation for our single neuron model:</p><p>$$z = x \\cdot w^T + w_0$$</p><p>$$a = f(z)$$</p><p>$$y = a$$</p><p>where $x$ and $w$ are row vectors, and the activation function $f$ operates element-wise on the vector $z.$ to produce a scalar $a.$</p>"
},
        {
          "id": "rergession-gradient-descent",
          "title": "Gradient Descent",
          "content": "<p>Gradient descent is an iterative optimization algorithm used to find the minimum of a function. In machine learning, it is the primary method for training models by minimizing a loss function, which measures the error between predicted and actual values</p>",
          "subsections": [
            {
              "id": "regression-gradient-descent",
              "title": "Regression - Gradient Descent with Squared Error(SE) Loss",
              "content": "<p>The activation function for the single neuron regression model will use a linear function $f(z) = z$ as the activation function. We would like to minimize a cost function, $J$, where $J$ is the total *loss* $L$ over our training data:</p><p>$$ J = \\sum_i^n L(\\hat{y}^{(i)}, y^{(i)})$$</p><p>For this simple example, we will use squared error (SE) loss where $\\epsilon^{(i)}$ is our error for any given sample $i$:</p><p>$$ L_{SE}(\\hat{y}^{(i)}, y^{(i)}) = \\frac{1}{2} (\\hat{y_i} - y_i)^2 = \\frac{1}{2} \\epsilon^2 $$</p><p>Additionally,</p><ul><li>We will perform the training loop for a specified number of iterations through our dataset, also known as epochs.</li><li>In each epoch, we will look at each input and output $(x^{(i)},y^{(i)})$ pair. For each pair, we:<ul><li>Calculate the loss $L$ between the correct value $y$ and the predicted value $\\hat{y}$</li><li>Calculate the gradient of the loss with respect to each weight, and</li><li>Update the weights based on the gradient and the learning rate, $\\eta$:</li></ul></li></ul><p>$$ w_{j,new} = w_j - \\eta \\frac{dJ}{dw_j}$$</p><p>Although we are implementing our Single Neuron Model in a class, the basic concept of gradient based learning stays the same. We are still nudging each weight along the gradient of the cost funtion with respect to that weight. However the location of where those weights is stored is different and within each instance of our class.</p><h3>Gradient descent</h3><p>In the case of our squared error loss, for any sample data point $i$ this works out to:</p><p>$$ \\frac{dJ}{dw_j} = \\frac{dL_{SE}}{dw_j}   \n  = \\frac{dL_{SE}}{d\\hat{y}} \\frac{d\\hat{y}}{dw_j} \n  = (\\hat{y} - y) \\frac{d\\hat{y}}{dw_j} $$</p><p>Importantly, we see that we need to be able to calculate the gradient of the model output $\\hat{y}$ with respect to each weight:</p><p>$$ \\frac{d\\hat{y}}{dw_j} = \\frac{df(z)}{dz} \\frac{dz}{dw_j} $$</p><p>For the linear activation function $f(z) = z$, the first term is very simple: $\\frac{df(z)}{dz} = 1$.</p><p>For the second term,</p><p>$$ z = w_0 + x_1 \\cdot w_1 + \\cdots + x_j \\cdot w_j + \\dots + x_m \\cdot w_m $$</p><p>so $\\frac{dz}{dw_j} = x_j$, except for $w_0$, where $\\frac{dz}{dw_0} = 1.$</p>"
            },
            {
              "id": "classification-gradient-descent",
              "title": "Classification - Gradient Descent with Negative Log-Likelihood (NLL) Loss",
              "content": "<p>For the classification problem, we will change the activation function to a sigmoid. The sigmoid function squashes the pre-activation $z$ down to an activation (output) that is between 0 and 1. We also need to implement the gradient calculation, with this sigmoid activation function.</p><h3>Gradient descent</h3><p>In the case of our NLL loss, for any sample data point $i$, the gradient of $J$ with respect to weights works out to:</p><p>$$ \\frac{dJ}{dw_j} = \\frac{dL_{NLL}}{dw_j} = \\frac{dL_{NLL}}{d\\hat{y}} \\frac{d\\hat{y}}{dw_j} = \\frac{dL_{NLL}}{d\\hat{y}} \\frac{d\\hat{y}}{dz} \\frac{dz}{dw_j} = \\frac{dL_{NLL}}{d\\hat{y}} \\frac{d\\sigma{(z)}}{dz} \\frac{dz}{dw_j} $$</p><p>For the first term, the definition $L_{NLL} = y \\log{\\hat{y}} + (1-y)\\log{(1-\\hat{y})}$, giving us the following after some algebra:</p><p>$$ \\frac{dL_{NLL}}{d\\hat{y}} = \\frac{\\hat{y} - y}{\\hat{y}(1-\\hat{y})} .$$</p><p>For the second term, our derivative of the activation function $\\sigma(z)$, we get:</p><p>$$ \\frac{d\\sigma{(z)}}{dz} = \\sigma(z)(1-\\sigma(z)  = \\hat{y}(1-\\hat{y}).$$</p><p>And finally, the last term is simply $\\frac{dz}{dw_j} = x$, except for $\\frac{dz}{dw_0} = 1$.</p>"
            }
          ]
          },

        {
          "id": "project-setup",
          "title": "Project Setup",
          "content": "<pre><code># Clone the repository\ngit clone https://github.com/Uvira007/single-neuron-model\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install dependencies\npip install -r requirements.txt</code></pre><p>Run the Single_Neuron_Model.ipynb notebook from top-to-bottom. Restart the kernel and clear the outputs, if the gradients explode. </p>"
        },
        {
          "id": "model-results",
          "title": "Model Results",
          "content": "<p>The regression model had a Mean Squared Error of 0.049 and the model fit is shown below. The classification model has an accuracy of 91.47%</p><div style='display: flex; justify-content: center; align-items: center; gap: 2rem; flex-wrap: wrap; margin-top: 1.5rem;'><img src='assets/projects/single-neuron-model/Regression-Result.png' alt='Regression Model Result' style='height: 320px; width: 350px; border-radius: 8px;'><img src='assets/projects/single-neuron-model/Classification-Result.png' alt='Classification Model Result' style='height: 320px; width: 350px; border-radius: 8px;'></div>"
        },
        {
          "id": "conclusion",
          "title": "Conclusion",
          "content": "<p>Now although classes are a very powerful tool, they are still not powerful enough to allow our Single Neuron Model to determine good decision boundaries for non linearly separable data. [Pytorch library](https://pytorch.org/) uses all of the afore mentioned concepts to create extremely sophisticated Neural Networks that will allow us to tackle more complex datasets.</p>"
        }
      ]
    },
    {
      "id": "ufo-analysis",
      "title": "Identifying UFO Sightings Patterns Using Text Analysis",
      "description": "Traditional NLP techniques and unsupervised machine learning algorithms are used in this project to analyze the trove of documents released by CIA on UFO sightings to identify hidden patterns.",
      "image": "assets/projects/ufo-analysis/base-image.png",
      "readTime": "12 minute read",
      "tags": ["K-Means", "elbow method", "silhouette coefficient", "NLP", "pdfminer", "cosine similarity", "Vector Space Model", "nltk", "tf-idf"],
      "sections": [
        {
          "id": "project-overview",
          "title": "Project Overview",
          "content": "<p>There has been a recent uptick in the conversations surrounding the notion of \"Unidentified Flying Objects,\" or UFOs, in the media due to Congressional inquiries on the topic. The United States Government Freedom of Information Act (FOIA) allows individual citizens the right to ask for and receive previously unreleased documents possessed by the Government upon request. When these documents are released according to the law, \"Internet detectives\" pour through the documents to find that hidden nugget of information. In the case of this project, we want to see if there are any hidden patterns surrounding the origin of these so-called UFOs that the Government hasn't disclosed before. We will apply our developed system to a series of documents released by the Central Intelligence Agency (CIA) through a FOIA request to better understand the nature of the data.</p>",
          "subsections": [
            {
              "id": "problem-description",
              "title": "Problem Description",
              "content": "<p>Some of the technical challenges of this project include:</p><ul><li><b>Large Troves of Documents.</b> When released, a trove of documents is published all at once. These can include hundreds of PDFs with thousands of pages. For this particular release of documents from the CIA, there are 712 documents composed of 3493 total pages.</li><li><b>Data That is Dirty.</b> The Government releases documents that are scans of printed materials, with redactions made. This is to ensure that nothing is accidentally revealed that isn't supposed to be, so a physical step is required in Government release procedures. As such, any useful digital representation that may have existed is obliterated. The scans tend not to be sophisticated or of high-quality</li><li><b>Unusual Lexicon.</b> A standard dictionary methodology may not be appropriate for certain document types, especially this dataset. The unusual lexicon surrounding both alien and terrestrial technology will require an unsupervised approach to performing processing.</li><li><b>No Categorization or Labeling.</b> Documents are released without any labeling or categorization of any kind. This leaves it as an exercise for the recipient to pour through the mounds of information to find the needle in the haystack to find the information.</li></ul>"
            },
            {
              "id": "actions",
              "title": "Actions",
              "content": "<p>This tool will be designed generically to accommodate any type of cache of FOIA-released documents from the Government in PDF form. Entire directories of documents will be processed within.</p><p>\nThere will be two primary user classes for the developed software. The first user class is the <i>data scientist</i> who will set up the backend for data processing, including administering the parameters for the model generation steps. The second user class is a <i>citizen data consumer</i> interested in understanding the document set, especially for finding the hidden patterns within the data. In addition, this user class will leverage the clusters that have been exposed.</p>"
            },
            {
              "id": "results",
              "title": "Results",
              "content": "<p>A k-value of 10 is chosen based on the elbow method and silhouette coefficient. The silhouette scores showed low confidence in the cluster assignment and it was reflected in the clusters during subsequent runs. There was variability in the cluster contents.</p><p>\n The query search implementation using Vector Space Model proved useful in finding similar documents for the given search text. One of the advantages of this application is that it is generalized to be used for documents from any domain, making it powerful to do initial analysis for NLP tasks.</p>"
            },
            {
              "id": "growth",
              "title": "Growth/Next Steps",
              "content": "<p>The application can be accessed via <a href=\"https://foia-on-ufo-analysis.streamlit.app/\" target=\"_blank\"><u>FOIA UFO Analysis</u></a>. Currently the application runs on streamlit community cloud with limited resources. The application can be expanded to run at-scale by utilizing a paid cloud services.</p>"
            }
          ]
        },
        {
          "id": "data-overview",
          "title": "Sample Data Overview",
          "content": "<p>A total of 712pdf documents, containing 3493 pages of text related to UFO were converted from the tiff images from the CIA. The document collection consists of a mixture of scans of printed materials with redactions, memos, transcripts and high quality report documents. A sample for low quality redacted document(left) and a high quality report document(right) is shown below</p><div style='display: flex; justify-content: center; align-items: center; gap: 7rem; flex-wrap: wrap; margin-top: 1.5rem;'><img src='assets/projects/ufo-analysis/low-quality-data-image.png' alt='low quality' style='height: 320px; width: 350px; border-radius: 8px;'><img src='assets/projects/ufo-analysis/high-quality-data-image.png' alt='High quality' style='height: 320px; width: 350px; border-radius: 8px;'></div>"
        },
        {
          "id": "system-architecture",
          "title": "System Architecture Overview",
          "content": "<p>The overall architecture will be designed around three subsystems and pipelines, the Backend Data Subsystem (in Blue), the Data Processing Pipeline (in Orange), and the Visualization Subsystem (in Green):</p><div style='display: flex; justify-content: center; align-items: center; gap: 2rem; flex-wrap: wrap; margin-top: 1.5rem;'><img src='assets/projects/ufo-analysis/architecture.PNG' alt='Architecture' style='border-radius: 8px;'></div><p><h3>Backend Data Subsystem</h3>The backend data subsystem will be responsible for opening all of the OCR'd PDF files within the specified directory, cleaning them as appropriate, and building the interim data format for the data processing pipeline.<ul><li>Documents are parsed and imported into memory.</li><li>All words will be compared to a Wordnet, and only words with semantics will be indexed.</li><li>Only documents that ultimately have more than two actual words will be indexed.</li></ul><p>The goal of these conditioning steps is to remove the numerous incorrectly recognized words in some of the more poorly scanned documents. This will result in a more accurate representation of the hidden patterns within the documents. Finally, as a stretch goal, we will have an accessor available so that a user can choose a specific document from a cluster displayed on the rendered visualizations and see the contents of the original (source) document that was used.</p><p><h3>Data Processing Pipeline</h3>The data processing pipeline will be responsible for building all elements required for indexing, querying, and building clusters of our data.<p><ul><li>Build features of the data set by leveraging a count of the words used in the documents.</li><li>Calculate the Sum of Squared distances for a range of clusters, likely from 2 to 20.</li><li>Determine the optimal number of clusters through analysis of the \"elbow method.\"</li><li>Cluster PDFs by fitting the model to the entire data set.</li></ul></p><p>This pipeline will allow the modification of a number of parameters around this clustering technique. A few examples of these include the initial cluster space, often done via random seeding. A second is the number of clusters that the documents will be fit to. Finally, the word frequency for a given cluster as presented to the user. In addition, this data processing pipeline will create all appropriate indexes so that querying can be accomplished through the web-based frontend.</p><p><h3>Visualization Subsystem</h3>The visualization subsystem will be responsible for displaying user interfaces that will allow for both the administering of the data processing pipeline parameters and also the visualization of the results of the analysis. Modifiable parameters in the User Interface will include:<p><ul><li>Cluster Size</li><li>Clustering Initialization Method</li><li>Number of Iterations</li><li>Feature Extraction Method</li></ul></p></p></p></p>"
        },
        {
          "id": "algorithm-overview",
          "title": "Algorithm Overview",
          "content": "<p>Our methodology employs a dual-layered analysis to navigate the document corpus. We first utilize <b>K-means clustering</b>, an unsupervised learning algorithm, to partition the documents into thematic groups and reveal inherent structural patterns. To complement this, we implement a <b>Vector Space Model</b> that transforms textual data into high-dimensional numerical vectors. By applying <b>TF-IDF weighting</b> and <b>cosine similarity</b>, the system enables precise search capabilities by measuring the angular distance between query and document vectors, ensuring that the most contextually relevant results are retrieved.</p><ul><li><b>K-means Clustering:</b> An iterative process that minimizes the distance between data points and a central \"centroid\" to form compact, separate groups of similar documents.</li><li><b>Vector Space Model (VSM):</b> A mathematical model where each document is treated as a vector in a multi-dimensional space, with each unique word representing a dimension.</li><li><b>TF-IDF & Cosine Similarity:</b> TF-IDF weights words based on their importance to a specific document versus the whole collection, while cosine similarity measures the angle between vectors to determine how \"close\" two documents are in meaning.</li>"
        },
        {
          "id": "project-setup",
          "title": "Project Setup",
          "content": "<p>The application is currently hosted on the Streamlit Community Cloud for immediate use. You may conduct your own analysis by uploading document sets directly to the <a href=\"https://foia-on-ufo-analysis.streamlit.app/\" target=\"_blank\"><u>FOIA UFO Analysis Portal</u></a>. Alternatively, for those who prefer a local environment, the following instructions outline the setup process for private deployment.</p><pre><code># Clone and setup\ngit clone https://github.com/Uvira007/foia-on-ufo-analysis\npip install -r requirements.txt\n\n# Ensure to run the command from the root of the project\nstreamlit run src/app/Welcome.py </code></pre>"
        },
          {
          "id": "model-results",
          "title": "Model Results",
          "content": "<p>The plot of elbow method to determine the optimal k value and the word cloud for one of the clusters formed is shown below.</p><div style='display: flex; justify-content: center; align-items: center; gap: 5rem; flex-wrap: wrap; margin-top: 1.5rem;'><img src='assets/projects/ufo-analysis/elbow-method-optimal-k.png' alt='elbow method' style='height: 250px; width: 450px; border-radius: 8px;'><img src='assets/projects/ufo-analysis/word-cloud.png' alt='Word Cloud' style='height: 250px; width: 450px; border-radius: 8px;'></div>"
        },
        {
          "id": "conclusion",
          "title": "Conclusion",
          "content": "<p>The fruit classification system provides an efficient solution for automating grocery checkout, with potential to significantly reduce customer wait times.</p>"
        }
      ]
    },
    {
      "id": "ab-testing",
      "title": "A/B Testing: Optimizing Website Conversion Rates",
      "description": "In this project we apply statistical analysis to determine whether a new website design increases customer conversion rates compared to the current design.",
      "image": "assets/projects/ab-testing.jpg",
      "readTime": "25 minute read",
      "tags": ["Statistics", "A/B Testing", "Data Science", "Python", "Hypothesis Testing"],
      "sections": [
        {
          "id": "project-overview",
          "title": "Project Overview",
          "content": "<p>This project demonstrates rigorous A/B testing methodology to evaluate website design changes and their impact on conversion rates.</p>",
          "subsections": [
            {
              "id": "context",
              "title": "Context",
              "content": "<p>The marketing team proposed a new landing page design. Before full rollout, we need statistical evidence that the new design actually improves conversions.</p>"
            },
            {
              "id": "actions",
              "title": "Actions",
              "content": "<p>We conducted a proper A/B test including:</p><ul><li>Sample size calculation for 80% statistical power</li><li>Random user assignment to control/treatment</li><li>Chi-square test for significance</li><li>Confidence interval estimation</li></ul>"
            },
            {
              "id": "results",
              "title": "Results",
              "content": "<p>The new design showed a <strong>15% relative improvement</strong> in conversion rate (p-value < 0.01), justifying full deployment.</p>"
            },
            {
              "id": "growth",
              "title": "Growth/Next Steps",
              "content": "<p>Implement multi-armed bandit approach for continuous optimization.</p>"
            }
          ]
        },
        {
          "id": "data-overview",
          "title": "Data Overview",
          "content": "<p>The test ran for 4 weeks with 50,000 users per group, ensuring sufficient statistical power to detect a 5% minimum detectable effect.</p>"
        },
        {
          "id": "algorithm-overview",
          "title": "Statistical Methods Overview",
          "content": "<p>We used chi-square tests for comparing proportions, with Bonferroni correction for multiple comparisons. Bootstrap methods were employed for confidence interval estimation.</p>"
        },
        {
          "id": "project-setup",
          "title": "Project Setup",
          "content": "<pre><code>pip install -r requirements.txt\npython ab_test_analysis.py</code></pre>"
        },
        {
          "id": "demo",
          "title": "Demo",
          "content": "<p>Interactive Jupyter notebook available for exploring the statistical analysis.</p>"
        },
        {
          "id": "conclusion",
          "title": "Conclusion",
          "content": "<p>Proper statistical testing is essential before making data-driven decisions. This framework can be applied to any A/B testing scenario.</p>"
        }
      ]
    },
    {
      "id": "causal-impact",
      "title": "Causal Impact Analysis: Measuring Marketing Effectiveness",
      "description": "In this project we apply causal inference techniques to measure the true impact of a marketing campaign on sales, controlling for external factors.",
      "image": "assets/projects/causal-impact.jpg",
      "readTime": "35 minute read",
      "tags": ["Causal Inference", "Time Series", "Data Science", "Python", "Bayesian"],
      "sections": [
        {
          "id": "project-overview",
          "title": "Project Overview",
          "content": "<p>This project uses Google's CausalImpact methodology to estimate the true effect of marketing interventions on business outcomes.</p>",
          "subsections": [
            {
              "id": "context",
              "title": "Context",
              "content": "<p>Traditional before/after comparisons can be misleading due to seasonality, trends, and external events. Causal inference provides more reliable estimates.</p>"
            },
            {
              "id": "actions",
              "title": "Actions",
              "content": "<p>We implemented Bayesian structural time series models to:</p><ul><li>Create synthetic control from related markets</li><li>Estimate counterfactual scenario</li><li>Calculate incremental impact with uncertainty</li></ul>"
            },
            {
              "id": "results",
              "title": "Results",
              "content": "<p>The campaign generated an estimated <strong>$2.3M incremental revenue</strong> (95% CI: $1.8M - $2.8M) over the 8-week period.</p>"
            },
            {
              "id": "growth",
              "title": "Growth/Next Steps",
              "content": "<p>Extend analysis to multiple marketing channels for attribution modeling.</p>"
            }
          ]
        },
        {
          "id": "data-overview",
          "title": "Data Overview",
          "content": "<p>Weekly sales data from 52 weeks pre-intervention and 8 weeks during the campaign, along with control market data for synthetic control construction.</p>"
        },
        {
          "id": "algorithm-overview",
          "title": "Methodology Overview",
          "content": "<p>Bayesian structural time series combines state space models with spike-and-slab priors for automatic variable selection, providing robust counterfactual predictions.</p>"
        },
        {
          "id": "project-setup",
          "title": "Project Setup",
          "content": "<pre><code>pip install pycausalimpact\npython causal_analysis.py</code></pre>"
        },
        {
          "id": "demo",
          "title": "Demo",
          "content": "<p>Visualization dashboard showing the actual vs. predicted counterfactual with confidence bands.</p>"
        },
        {
          "id": "conclusion",
          "title": "Conclusion",
          "content": "<p>Causal impact analysis provides a rigorous framework for measuring marketing ROI beyond simple correlation.</p>"
        }
      ]
    },
    {
      "id": "customer-segmentation",
      "title": "Customer Segmentation Using K-Means Clustering",
      "description": "In this project we use unsupervised learning to segment customers based on purchasing behavior, enabling targeted marketing strategies.",
      "image": "assets/projects/segmentation.jpg",
      "readTime": "20 minute read",
      "tags": ["Machine Learning", "Clustering", "Data Science", "Python", "K-Means"],
      "sections": [
        {
          "id": "project-overview",
          "title": "Project Overview",
          "content": "<p>This project applies K-Means clustering to identify distinct customer segments from transaction data, enabling personalized marketing.</p>",
          "subsections": [
            {
              "id": "context",
              "title": "Context",
              "content": "<p>One-size-fits-all marketing is inefficient. Understanding different customer segments allows for more effective targeting and messaging.</p>"
            },
            {
              "id": "actions",
              "title": "Actions",
              "content": "<p>Our analysis included:</p><ul><li>RFM (Recency, Frequency, Monetary) feature engineering</li><li>Elbow method and silhouette analysis for optimal K</li><li>Cluster profiling and interpretation</li><li>Actionable segment-specific strategies</li></ul>"
            },
            {
              "id": "results",
              "title": "Results",
              "content": "<p>Identified <strong>5 distinct customer segments</strong> with clear behavioral profiles, enabling a 23% improvement in campaign response rates.</p>"
            },
            {
              "id": "growth",
              "title": "Growth/Next Steps",
              "content": "<p>Implement real-time segment assignment for new customers and monitor segment migration over time.</p>"
            }
          ]
        },
        {
          "id": "data-overview",
          "title": "Data Overview",
          "content": "<p>Transaction history from 10,000 customers over 2 years, including purchase dates, amounts, and product categories.</p>"
        },
        {
          "id": "algorithm-overview",
          "title": "K-Means Algorithm Overview",
          "content": "<p>K-Means partitions data into K clusters by minimizing within-cluster variance. We used standardized features and multiple random initializations for stability.</p>"
        },
        {
          "id": "project-setup",
          "title": "Project Setup",
          "content": "<pre><code>pip install -r requirements.txt\npython segmentation_analysis.py</code></pre>"
        },
        {
          "id": "demo",
          "title": "Demo",
          "content": "<p>Interactive visualization of customer segments with drill-down capabilities.</p>"
        },
        {
          "id": "conclusion",
          "title": "Conclusion",
          "content": "<p>Customer segmentation provides actionable insights for marketing personalization and resource allocation.</p>"
        }
      ]
    },
    {
      "id": "association-rules",
      "title": "Market Basket Analysis Using Association Rules",
      "description": "In this project we discover which products are frequently purchased together, enabling strategic product placement and cross-selling opportunities.",
      "image": "assets/projects/market-basket.jpg",
      "readTime": "18 minute read",
      "tags": ["Data Mining", "Association Rules", "Data Science", "Python", "Apriori"],
      "sections": [
        {
          "id": "project-overview",
          "title": "Project Overview",
          "content": "<p>This project uses the Apriori algorithm to discover product associations in retail transaction data.</p>",
          "subsections": [
            {
              "id": "context",
              "title": "Context",
              "content": "<p>Understanding which products are bought together enables better store layout, promotions, and recommendation systems.</p>"
            },
            {
              "id": "actions",
              "title": "Actions",
              "content": "<p>We performed:</p><ul><li>Transaction data preprocessing</li><li>Apriori algorithm for frequent itemset mining</li><li>Association rule generation with support/confidence thresholds</li><li>Network visualization of product relationships</li></ul>"
            },
            {
              "id": "results",
              "title": "Results",
              "content": "<p>Discovered <strong>47 high-confidence rules</strong> with lift > 3, leading to store layout optimizations that increased basket size by 8%.</p>"
            },
            {
              "id": "growth",
              "title": "Growth/Next Steps",
              "content": "<p>Extend to online recommendation engine and seasonal pattern analysis.</p>"
            }
          ]
        },
        {
          "id": "data-overview",
          "title": "Data Overview",
          "content": "<p>Transaction data from 50,000 shopping baskets containing 200 unique products.</p>"
        },
        {
          "id": "algorithm-overview",
          "title": "Apriori Algorithm Overview",
          "content": "<p>Apriori efficiently finds frequent itemsets by pruning candidates that contain infrequent subsets. Rules are then generated with configurable support and confidence thresholds.</p>"
        },
        {
          "id": "project-setup",
          "title": "Project Setup",
          "content": "<pre><code>pip install mlxtend\npython basket_analysis.py</code></pre>"
        },
        {
          "id": "demo",
          "title": "Demo",
          "content": "<p>Interactive network graph showing product relationships and association strengths.</p>"
        },
        {
          "id": "conclusion",
          "title": "Conclusion",
          "content": "<p>Association rule mining reveals hidden patterns in transaction data that drive practical business decisions.</p>"
        }
      ]
    }
  ]
}
