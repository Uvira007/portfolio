{
  "projects": [
    {
      "id": "single-neuron-model",
      "title": "Single Neuron Model for Classification and Regression",
      "description": "In this project a single neuron model is implemented that can be used for both classification and regression tasks",
      "image": "assets\\projects\\single-neuron-model\\single-neuron-model.gif",
      "readTime": "15 minute read",
      "tags": ["Perceptron", "Classification","Regression", "Gradient Descent", "Optimization perspective"],
      "sections": [
        {
          "id": "project-overview",
          "title": "Project Overview",
          "content": "<p>This project focuses on building a single neuron model that uses gradient descent with back propogation. A linear activation function is used for Regression and a Sigmoid function is used for Classification</p>",
          "subsections": [
            {
              "id": "context",
              "title": "Context",
              "content": "<p>The history of Artificial Intelligence traces way back to 1943 on a paper called <a href=\"https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf\" target=\"_blank\">A Logical Calculus of the Ideas Immanent in Nervous Activity</a>. From a single Neuron -> Single layer network -> Multi Layer Perceptron, the development in the space of machine learning has come a long way. This project aims at developing the fundamental unit or the basic building block in artificial neural network called the neuron.</p>"
            },
            {
              "id": "actions",
              "title": "Actions",
              "content": "<p>The implementation of the single neuron model is as follows:</p><ul><li>A base class for single neuron model that implements the forward function and initialize weights</li><li>Single Neuron regression Model sub class that implements the activation and gradient function</li><li>A single neuron classifcation model sub class that implements the activation and gradient function</li></ul>"
            },
            {
              "id": "results",
              "title": "Results",
              "content": "<p>An agnostic single neuron model that can be used for any classification and regression model. This shows the power of a single neuron which when implemented in deep layers can produce astonishing results</p>"
            },
            {
              "id": "growth",
              "title": "Growth/Next Steps",
              "content": "<p>Although this model is for educational purposes,</p><ul><li>This model can be extended to a single layer neural network with multiple neurons</li><li>The single layer can further be extended to produce a multi layer perceptron capable of producing impressive results by creating complex decision boundaries.</li></ul>"
            }
          ]
        },
        {
          "id": "data-overview",
          "title": "Sample Data Overview",
          "content": "<p>The model can be used for both classification and regression tasks.</p><ul><li>For the classification task, wine quality dataset is used</li><li>For the regression task, advertising dataset is used</li></ul>"
        },
        {
          "id": "algorithm-overview",
          "title": "Single Neuron Model for Classification and Regression",
          "content": "<p>Our single neuron models consist of a single neuron or node, that sums weighted multiplications of the features of an input sample, adds a bias term, and then passes that sum through some activation function. For regression, we will use a linear activation function, i.e., just the identity function. For classification, we will use a sigmoid function.</p><p>More specifically, our single neuron model will take the dot product of an input training example $x^{(i)}$ with some learned weights $w$ and adding a learned bias $w_0$ to produce a pre-activation $z$. We then apply some activation function $f$ to $z$ to produce an activation $a$, which for this single neuron will be our output prediction $\\hat{y}$. Importantly, $x$ and $w$ can be vectors, which we will now represent as NumPy arrays in our Python code. We will use a subscript notation, e.g., $x_j$ to indicate feature $j$ within data input $x$, where $j$ goes from 1 to $m$ total features.</p><p>The following is the formal mathematical notation for our single neuron model:</p><p>$$z = x \\cdot w^T + w_0$$</p><p>$$a = f(z)$$</p><p>$$y = a$$</p><p>where $x$ and $w$ are row vectors, and the activation function $f$ operates element-wise on the vector $z.$ to produce a scalar $a.$</p>"
},
        {
          "id": "rergession-gradient-descent",
          "title": "Gradient Descent",
          "content": "<p>Gradient descent is an iterative optimization algorithm used to find the minimum of a function. In machine learning, it is the primary method for training models by minimizing a loss function, which measures the error between predicted and actual values</p>",
          "subsections": [
            {
              "id": "regression-gradient-descent",
              "title": "Regression - Gradient Descent with Squared Error(SE) Loss",
              "content": "<p>The activation function for the single neuron regression model will use a linear function $f(z) = z$ as the activation function. We would like to minimize a cost function, $J$, where $J$ is the total *loss* $L$ over our training data:</p><p>$$ J = \\sum_i^n L(\\hat{y}^{(i)}, y^{(i)})$$</p><p>For this simple example, we will use squared error (SE) loss where $\\epsilon^{(i)}$ is our error for any given sample $i$:</p><p>$$ L_{SE}(\\hat{y}^{(i)}, y^{(i)}) = \\frac{1}{2} (\\hat{y_i} - y_i)^2 = \\frac{1}{2} \\epsilon^2 $$</p><p>Additionally,</p><ul><li>We will perform the training loop for a specified number of iterations through our dataset, also known as epochs.</li><li>In each epoch, we will look at each input and output $(x^{(i)},y^{(i)})$ pair. For each pair, we:<ul><li>Calculate the loss $L$ between the correct value $y$ and the predicted value $\\hat{y}$</li><li>Calculate the gradient of the loss with respect to each weight, and</li><li>Update the weights based on the gradient and the learning rate, $\\eta$:</li></ul></li></ul><p>$$ w_{j,new} = w_j - \\eta \\frac{dJ}{dw_j}$$</p><p>Although we are implementing our Single Neuron Model in a class, the basic concept of gradient based learning stays the same. We are still nudging each weight along the gradient of the cost funtion with respect to that weight. However the location of where those weights is stored is different and within each instance of our class.</p><h3>Gradient descent</h3><p>In the case of our squared error loss, for any sample data point $i$ this works out to:</p><p>$$ \\frac{dJ}{dw_j} = \\frac{dL_{SE}}{dw_j}   \n  = \\frac{dL_{SE}}{d\\hat{y}} \\frac{d\\hat{y}}{dw_j} \n  = (\\hat{y} - y) \\frac{d\\hat{y}}{dw_j} $$</p><p>Importantly, we see that we need to be able to calculate the gradient of the model output $\\hat{y}$ with respect to each weight:</p><p>$$ \\frac{d\\hat{y}}{dw_j} = \\frac{df(z)}{dz} \\frac{dz}{dw_j} $$</p><p>For the linear activation function $f(z) = z$, the first term is very simple: $\\frac{df(z)}{dz} = 1$.</p><p>For the second term,</p><p>$$ z = w_0 + x_1 \\cdot w_1 + \\cdots + x_j \\cdot w_j + \\dots + x_m \\cdot w_m $$</p><p>so $\\frac{dz}{dw_j} = x_j$, except for $w_0$, where $\\frac{dz}{dw_0} = 1.$</p>"
            },
            {
              "id": "classification-gradient-descent",
              "title": "Classification - Gradient Descent with Negative Log-Likelihood (NLL) Loss",
              "content": "<p>For the classification problem, we will change the activation function to a sigmoid. The sigmoid function squashes the pre-activation $z$ down to an activation (output) that is between 0 and 1. We also need to implement the gradient calculation, with this sigmoid activation function.</p><h3>Gradient descent</h3><p>In the case of our NLL loss, for any sample data point $i$, the gradient of $J$ with respect to weights works out to:</p><p>$$ \\frac{dJ}{dw_j} = \\frac{dL_{NLL}}{dw_j} = \\frac{dL_{NLL}}{d\\hat{y}} \\frac{d\\hat{y}}{dw_j} = \\frac{dL_{NLL}}{d\\hat{y}} \\frac{d\\hat{y}}{dz} \\frac{dz}{dw_j} = \\frac{dL_{NLL}}{d\\hat{y}} \\frac{d\\sigma{(z)}}{dz} \\frac{dz}{dw_j} $$</p><p>For the first term, the definition $L_{NLL} = y \\log{\\hat{y}} + (1-y)\\log{(1-\\hat{y})}$, giving us the following after some algebra:</p><p>$$ \\frac{dL_{NLL}}{d\\hat{y}} = \\frac{\\hat{y} - y}{\\hat{y}(1-\\hat{y})} .$$</p><p>For the second term, our derivative of the activation function $\\sigma(z)$, we get:</p><p>$$ \\frac{d\\sigma{(z)}}{dz} = \\sigma(z)(1-\\sigma(z)  = \\hat{y}(1-\\hat{y}).$$</p><p>And finally, the last term is simply $\\frac{dz}{dw_j} = x$, except for $\\frac{dz}{dw_0} = 1$.</p>"
            }
          ]
          },

        {
          "id": "project-setup",
          "title": "Project Setup",
          "content": "<pre><code># Clone the repository\ngit clone https://github.com/Uvira007/single-neuron-model\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install dependencies\npip install -r requirements.txt</code></pre><p>Run the Single_Neuron_Model.ipynb notebook from top-to-bottom. Restart the kernel and clear the outputs, if the gradients explode. </p>"
        },
        {
          "id": "model-results",
          "title": "Model Results",
          "content": "<p>The regression model had a Mean Squared Error of 0.049 and the model fit is shown below. The classification model has an accuracy of 91.47%</p><div style='display: flex; justify-content: center; align-items: center; gap: 2rem; flex-wrap: wrap; margin-top: 1.5rem;'><img src='assets/projects/single-neuron-model/Regression-Result.png' alt='Regression Model Result' style='height: 320px; width: 350px; border-radius: 8px;'><img src='assets/projects/single-neuron-model/Classification-Result.png' alt='Classification Model Result' style='height: 320px; width: 350px; border-radius: 8px;'></div>"
        },
        {
          "id": "conclusion",
          "title": "Conclusion",
          "content": "<p>This project demonstrates how deep learning can be applied to create practical e-commerce solutions. The visual search engine provides an intuitive way for customers to find products, potentially increasing engagement and sales.</p>"
        }
      ]
    },
    {
      "id": "fruit-classification",
      "title": "Fruit Classification Using A Convolutional Neural Network",
      "description": "In this project we build & optimise a Convolutional Neural Network to classify images of fruits, with the goal of helping a grocery retailer enhance their self-checkout process.",
      "image": "assets/projects/fruit-classification.jpg",
      "readTime": "48 minute read",
      "tags": ["Deep Learning", "CNN", "Data Science", "Computer Vision", "Python"],
      "sections": [
        {
          "id": "project-overview",
          "title": "Project Overview",
          "content": "<p>This project develops an automated fruit classification system for grocery store self-checkout. The system identifies fruit types from camera images, eliminating the need for manual PLU code lookup.</p>",
          "subsections": [
            {
              "id": "context",
              "title": "Context",
              "content": "<p>Self-checkout stations often require customers to manually search for fruit PLU codes, which is time-consuming and error-prone. Automated image recognition can streamline this process significantly.</p>"
            },
            {
              "id": "actions",
              "title": "Actions",
              "content": "<p>Our approach included:</p><ul><li>Collected and augmented a dataset of 10 fruit categories</li><li>Built custom CNN architecture optimized for speed</li><li>Implemented data augmentation to improve generalization</li><li>Optimized model for edge deployment</li></ul>"
            },
            {
              "id": "results",
              "title": "Results",
              "content": "<p>Achieved <strong>97.3% classification accuracy</strong> across 10 fruit categories with inference time under 50ms on standard hardware.</p>"
            },
            {
              "id": "growth",
              "title": "Growth/Next Steps",
              "content": "<p>Next steps include expanding to 50+ fruit varieties and implementing quality assessment features.</p>"
            }
          ]
        },
        {
          "id": "data-overview",
          "title": "Sample Data Overview",
          "content": "<p>The dataset contains 5,000 images across 10 fruit categories: apples, bananas, oranges, avocados, kiwis, lemons, mangos, grapes, strawberries, and watermelons.</p><p>Images were captured under various lighting conditions and angles to ensure robust real-world performance.</p>"
        },
        {
          "id": "algorithm-overview",
          "title": "CNN Architecture Overview",
          "content": "<p>We designed a lightweight CNN with 5 convolutional blocks, each containing convolution, batch normalization, and max pooling layers. The architecture balances accuracy with inference speed for real-time applications.</p>"
        },
        {
          "id": "project-setup",
          "title": "Project Setup",
          "content": "<pre><code># Clone and setup\ngit clone https://github.com/yourusername/fruit-classifier.git\npip install -r requirements.txt\n\n# Train the model\npython train.py --epochs 50 --batch-size 32\n\n# Run inference\npython predict.py --image path/to/fruit.jpg</code></pre>"
        },
        {
          "id": "demo",
          "title": "Demo",
          "content": "<p>A live demo is available showing real-time fruit classification from webcam feed.</p>"
        },
        {
          "id": "conclusion",
          "title": "Conclusion",
          "content": "<p>The fruit classification system provides an efficient solution for automating grocery checkout, with potential to significantly reduce customer wait times.</p>"
        }
      ]
    },
    {
      "id": "ab-testing",
      "title": "A/B Testing: Optimizing Website Conversion Rates",
      "description": "In this project we apply statistical analysis to determine whether a new website design increases customer conversion rates compared to the current design.",
      "image": "assets/projects/ab-testing.jpg",
      "readTime": "25 minute read",
      "tags": ["Statistics", "A/B Testing", "Data Science", "Python", "Hypothesis Testing"],
      "sections": [
        {
          "id": "project-overview",
          "title": "Project Overview",
          "content": "<p>This project demonstrates rigorous A/B testing methodology to evaluate website design changes and their impact on conversion rates.</p>",
          "subsections": [
            {
              "id": "context",
              "title": "Context",
              "content": "<p>The marketing team proposed a new landing page design. Before full rollout, we need statistical evidence that the new design actually improves conversions.</p>"
            },
            {
              "id": "actions",
              "title": "Actions",
              "content": "<p>We conducted a proper A/B test including:</p><ul><li>Sample size calculation for 80% statistical power</li><li>Random user assignment to control/treatment</li><li>Chi-square test for significance</li><li>Confidence interval estimation</li></ul>"
            },
            {
              "id": "results",
              "title": "Results",
              "content": "<p>The new design showed a <strong>15% relative improvement</strong> in conversion rate (p-value < 0.01), justifying full deployment.</p>"
            },
            {
              "id": "growth",
              "title": "Growth/Next Steps",
              "content": "<p>Implement multi-armed bandit approach for continuous optimization.</p>"
            }
          ]
        },
        {
          "id": "data-overview",
          "title": "Data Overview",
          "content": "<p>The test ran for 4 weeks with 50,000 users per group, ensuring sufficient statistical power to detect a 5% minimum detectable effect.</p>"
        },
        {
          "id": "algorithm-overview",
          "title": "Statistical Methods Overview",
          "content": "<p>We used chi-square tests for comparing proportions, with Bonferroni correction for multiple comparisons. Bootstrap methods were employed for confidence interval estimation.</p>"
        },
        {
          "id": "project-setup",
          "title": "Project Setup",
          "content": "<pre><code>pip install -r requirements.txt\npython ab_test_analysis.py</code></pre>"
        },
        {
          "id": "demo",
          "title": "Demo",
          "content": "<p>Interactive Jupyter notebook available for exploring the statistical analysis.</p>"
        },
        {
          "id": "conclusion",
          "title": "Conclusion",
          "content": "<p>Proper statistical testing is essential before making data-driven decisions. This framework can be applied to any A/B testing scenario.</p>"
        }
      ]
    },
    {
      "id": "causal-impact",
      "title": "Causal Impact Analysis: Measuring Marketing Effectiveness",
      "description": "In this project we apply causal inference techniques to measure the true impact of a marketing campaign on sales, controlling for external factors.",
      "image": "assets/projects/causal-impact.jpg",
      "readTime": "35 minute read",
      "tags": ["Causal Inference", "Time Series", "Data Science", "Python", "Bayesian"],
      "sections": [
        {
          "id": "project-overview",
          "title": "Project Overview",
          "content": "<p>This project uses Google's CausalImpact methodology to estimate the true effect of marketing interventions on business outcomes.</p>",
          "subsections": [
            {
              "id": "context",
              "title": "Context",
              "content": "<p>Traditional before/after comparisons can be misleading due to seasonality, trends, and external events. Causal inference provides more reliable estimates.</p>"
            },
            {
              "id": "actions",
              "title": "Actions",
              "content": "<p>We implemented Bayesian structural time series models to:</p><ul><li>Create synthetic control from related markets</li><li>Estimate counterfactual scenario</li><li>Calculate incremental impact with uncertainty</li></ul>"
            },
            {
              "id": "results",
              "title": "Results",
              "content": "<p>The campaign generated an estimated <strong>$2.3M incremental revenue</strong> (95% CI: $1.8M - $2.8M) over the 8-week period.</p>"
            },
            {
              "id": "growth",
              "title": "Growth/Next Steps",
              "content": "<p>Extend analysis to multiple marketing channels for attribution modeling.</p>"
            }
          ]
        },
        {
          "id": "data-overview",
          "title": "Data Overview",
          "content": "<p>Weekly sales data from 52 weeks pre-intervention and 8 weeks during the campaign, along with control market data for synthetic control construction.</p>"
        },
        {
          "id": "algorithm-overview",
          "title": "Methodology Overview",
          "content": "<p>Bayesian structural time series combines state space models with spike-and-slab priors for automatic variable selection, providing robust counterfactual predictions.</p>"
        },
        {
          "id": "project-setup",
          "title": "Project Setup",
          "content": "<pre><code>pip install pycausalimpact\npython causal_analysis.py</code></pre>"
        },
        {
          "id": "demo",
          "title": "Demo",
          "content": "<p>Visualization dashboard showing the actual vs. predicted counterfactual with confidence bands.</p>"
        },
        {
          "id": "conclusion",
          "title": "Conclusion",
          "content": "<p>Causal impact analysis provides a rigorous framework for measuring marketing ROI beyond simple correlation.</p>"
        }
      ]
    },
    {
      "id": "customer-segmentation",
      "title": "Customer Segmentation Using K-Means Clustering",
      "description": "In this project we use unsupervised learning to segment customers based on purchasing behavior, enabling targeted marketing strategies.",
      "image": "assets/projects/segmentation.jpg",
      "readTime": "20 minute read",
      "tags": ["Machine Learning", "Clustering", "Data Science", "Python", "K-Means"],
      "sections": [
        {
          "id": "project-overview",
          "title": "Project Overview",
          "content": "<p>This project applies K-Means clustering to identify distinct customer segments from transaction data, enabling personalized marketing.</p>",
          "subsections": [
            {
              "id": "context",
              "title": "Context",
              "content": "<p>One-size-fits-all marketing is inefficient. Understanding different customer segments allows for more effective targeting and messaging.</p>"
            },
            {
              "id": "actions",
              "title": "Actions",
              "content": "<p>Our analysis included:</p><ul><li>RFM (Recency, Frequency, Monetary) feature engineering</li><li>Elbow method and silhouette analysis for optimal K</li><li>Cluster profiling and interpretation</li><li>Actionable segment-specific strategies</li></ul>"
            },
            {
              "id": "results",
              "title": "Results",
              "content": "<p>Identified <strong>5 distinct customer segments</strong> with clear behavioral profiles, enabling a 23% improvement in campaign response rates.</p>"
            },
            {
              "id": "growth",
              "title": "Growth/Next Steps",
              "content": "<p>Implement real-time segment assignment for new customers and monitor segment migration over time.</p>"
            }
          ]
        },
        {
          "id": "data-overview",
          "title": "Data Overview",
          "content": "<p>Transaction history from 10,000 customers over 2 years, including purchase dates, amounts, and product categories.</p>"
        },
        {
          "id": "algorithm-overview",
          "title": "K-Means Algorithm Overview",
          "content": "<p>K-Means partitions data into K clusters by minimizing within-cluster variance. We used standardized features and multiple random initializations for stability.</p>"
        },
        {
          "id": "project-setup",
          "title": "Project Setup",
          "content": "<pre><code>pip install -r requirements.txt\npython segmentation_analysis.py</code></pre>"
        },
        {
          "id": "demo",
          "title": "Demo",
          "content": "<p>Interactive visualization of customer segments with drill-down capabilities.</p>"
        },
        {
          "id": "conclusion",
          "title": "Conclusion",
          "content": "<p>Customer segmentation provides actionable insights for marketing personalization and resource allocation.</p>"
        }
      ]
    },
    {
      "id": "association-rules",
      "title": "Market Basket Analysis Using Association Rules",
      "description": "In this project we discover which products are frequently purchased together, enabling strategic product placement and cross-selling opportunities.",
      "image": "assets/projects/market-basket.jpg",
      "readTime": "18 minute read",
      "tags": ["Data Mining", "Association Rules", "Data Science", "Python", "Apriori"],
      "sections": [
        {
          "id": "project-overview",
          "title": "Project Overview",
          "content": "<p>This project uses the Apriori algorithm to discover product associations in retail transaction data.</p>",
          "subsections": [
            {
              "id": "context",
              "title": "Context",
              "content": "<p>Understanding which products are bought together enables better store layout, promotions, and recommendation systems.</p>"
            },
            {
              "id": "actions",
              "title": "Actions",
              "content": "<p>We performed:</p><ul><li>Transaction data preprocessing</li><li>Apriori algorithm for frequent itemset mining</li><li>Association rule generation with support/confidence thresholds</li><li>Network visualization of product relationships</li></ul>"
            },
            {
              "id": "results",
              "title": "Results",
              "content": "<p>Discovered <strong>47 high-confidence rules</strong> with lift > 3, leading to store layout optimizations that increased basket size by 8%.</p>"
            },
            {
              "id": "growth",
              "title": "Growth/Next Steps",
              "content": "<p>Extend to online recommendation engine and seasonal pattern analysis.</p>"
            }
          ]
        },
        {
          "id": "data-overview",
          "title": "Data Overview",
          "content": "<p>Transaction data from 50,000 shopping baskets containing 200 unique products.</p>"
        },
        {
          "id": "algorithm-overview",
          "title": "Apriori Algorithm Overview",
          "content": "<p>Apriori efficiently finds frequent itemsets by pruning candidates that contain infrequent subsets. Rules are then generated with configurable support and confidence thresholds.</p>"
        },
        {
          "id": "project-setup",
          "title": "Project Setup",
          "content": "<pre><code>pip install mlxtend\npython basket_analysis.py</code></pre>"
        },
        {
          "id": "demo",
          "title": "Demo",
          "content": "<p>Interactive network graph showing product relationships and association strengths.</p>"
        },
        {
          "id": "conclusion",
          "title": "Conclusion",
          "content": "<p>Association rule mining reveals hidden patterns in transaction data that drive practical business decisions.</p>"
        }
      ]
    }
  ]
}
