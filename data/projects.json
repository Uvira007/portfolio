{
  "projects": [
    {
      "id": "single-neuron-model",
      "title": "Single Neuron Model for Classification and Regression",
      "description": "In this project a single neuron model is implemented that can be used for both classification and regression tasks",
      "image": "assets\\projects\\single-neuron-model\\single-neuron-model.gif",
      "readTime": "15 minute read",
      "tags": ["Perceptron", "Classification","Regression", "Gradient Descent", "Optimization perspective"],
      "sections": [
        {
          "id": "project-overview",
          "title": "Project Overview",
          "content": "<p>This project focuses on building a single neuron model that uses gradient descent with back propogation. A linear activation function is used for Regression and a Sigmoid function is used for Classification</p>",
          "subsections": [
            {
              "id": "context",
              "title": "Context",
              "content": "<p>The history of Artificial Intelligence traces way back to 1943 on a paper called <a href=\"https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf\" target=\"_blank\">A Logical Calculus of the Ideas Immanent in Nervous Activity</a>. From a single Neuron -> Single layer network -> Multi Layer Perceptron, the development in the space of machine learning has come a long way. This project aims at developing the fundamental unit or the basic building block in artificial neural network called the neuron.</p>"
            },
            {
              "id": "actions",
              "title": "Actions",
              "content": "<p>The implementation of the single neuron model is as follows:</p><ul><li>A base class for single neuron model that implements the forward function and initialize weights</li><li>Single Neuron regression Model sub class that implements the activation and gradient function</li><li>A single neuron classifcation model sub class that implements the activation and gradient function</li></ul>"
            },
            {
              "id": "results",
              "title": "Results",
              "content": "<p>An agnostic single neuron model that can be used for any classification and regression model. This shows the power of a single neuron which when implemented in deep layers can produce astonishing results</p>"
            },
            {
              "id": "growth",
              "title": "Growth/Next Steps",
              "content": "<p>Although this model is for educational purposes,</p><ul><li>This model can be extended to a single layer neural network with multiple neurons</li><li>The single layer can further be extended to produce a multi layer perceptron capable of producing impressive results by creating complex decision boundaries.</li></ul>"
            }
          ]
        },
        {
          "id": "data-overview",
          "title": "Sample Data Overview",
          "content": "<p>The model can be used for both classification and regression tasks.</p><ul><li>For the classification task, wine quality dataset is used</li><li>For the regression task, advertising dataset is used</li></ul>"
        },
        {
          "id": "algorithm-overview",
          "title": "Single Neuron Model for Classification and Regression",
          "content": "<p>Our single neuron models consist of a single neuron or node, that sums weighted multiplications of the features of an input sample, adds a bias term, and then passes that sum through some activation function. For regression, we will use a linear activation function, i.e., just the identity function. For classification, we will use a sigmoid function.</p><p>More specifically, our single neuron model will take the dot product of an input training example $x^{(i)}$ with some learned weights $w$ and adding a learned bias $w_0$ to produce a pre-activation $z$. We then apply some activation function $f$ to $z$ to produce an activation $a$, which for this single neuron will be our output prediction $\\hat{y}$. Importantly, $x$ and $w$ can be vectors, which we will now represent as NumPy arrays in our Python code. We will use a subscript notation, e.g., $x_j$ to indicate feature $j$ within data input $x$, where $j$ goes from 1 to $m$ total features.</p><p>The following is the formal mathematical notation for our single neuron model:</p><p>$$z = x \\cdot w^T + w_0$$</p><p>$$a = f(z)$$</p><p>$$y = a$$</p><p>where $x$ and $w$ are row vectors, and the activation function $f$ operates element-wise on the vector $z.$ to produce a scalar $a.$</p>"
},
        {
          "id": "rergession-gradient-descent",
          "title": "Gradient Descent",
          "content": "<p>Gradient descent is an iterative optimization algorithm used to find the minimum of a function. In machine learning, it is the primary method for training models by minimizing a loss function, which measures the error between predicted and actual values</p>",
          "subsections": [
            {
              "id": "regression-gradient-descent",
              "title": "Regression - Gradient Descent with Squared Error(SE) Loss",
              "content": "<p>The activation function for the single neuron regression model will use a linear function $f(z) = z$ as the activation function. We would like to minimize a cost function, $J$, where $J$ is the total *loss* $L$ over our training data:</p><p>$$ J = \\sum_i^n L(\\hat{y}^{(i)}, y^{(i)})$$</p><p>For this simple example, we will use squared error (SE) loss where $\\epsilon^{(i)}$ is our error for any given sample $i$:</p><p>$$ L_{SE}(\\hat{y}^{(i)}, y^{(i)}) = \\frac{1}{2} (\\hat{y_i} - y_i)^2 = \\frac{1}{2} \\epsilon^2 $$</p><p>Additionally,</p><ul><li>We will perform the training loop for a specified number of iterations through our dataset, also known as epochs.</li><li>In each epoch, we will look at each input and output $(x^{(i)},y^{(i)})$ pair. For each pair, we:<ul><li>Calculate the loss $L$ between the correct value $y$ and the predicted value $\\hat{y}$</li><li>Calculate the gradient of the loss with respect to each weight, and</li><li>Update the weights based on the gradient and the learning rate, $\\eta$:</li></ul></li></ul><p>$$ w_{j,new} = w_j - \\eta \\frac{dJ}{dw_j}$$</p><p>Although we are implementing our Single Neuron Model in a class, the basic concept of gradient based learning stays the same. We are still nudging each weight along the gradient of the cost funtion with respect to that weight. However the location of where those weights is stored is different and within each instance of our class.</p><h3>Gradient descent</h3><p>In the case of our squared error loss, for any sample data point $i$ this works out to:</p><p>$$ \\frac{dJ}{dw_j} = \\frac{dL_{SE}}{dw_j}   \n  = \\frac{dL_{SE}}{d\\hat{y}} \\frac{d\\hat{y}}{dw_j} \n  = (\\hat{y} - y) \\frac{d\\hat{y}}{dw_j} $$</p><p>Importantly, we see that we need to be able to calculate the gradient of the model output $\\hat{y}$ with respect to each weight:</p><p>$$ \\frac{d\\hat{y}}{dw_j} = \\frac{df(z)}{dz} \\frac{dz}{dw_j} $$</p><p>For the linear activation function $f(z) = z$, the first term is very simple: $\\frac{df(z)}{dz} = 1$.</p><p>For the second term,</p><p>$$ z = w_0 + x_1 \\cdot w_1 + \\cdots + x_j \\cdot w_j + \\dots + x_m \\cdot w_m $$</p><p>so $\\frac{dz}{dw_j} = x_j$, except for $w_0$, where $\\frac{dz}{dw_0} = 1.$</p>"
            },
            {
              "id": "classification-gradient-descent",
              "title": "Classification - Gradient Descent with Negative Log-Likelihood (NLL) Loss",
              "content": "<p>For the classification problem, we will change the activation function to a sigmoid. The sigmoid function squashes the pre-activation $z$ down to an activation (output) that is between 0 and 1. We also need to implement the gradient calculation, with this sigmoid activation function.</p><h3>Gradient descent</h3><p>In the case of our NLL loss, for any sample data point $i$, the gradient of $J$ with respect to weights works out to:</p><p>$$ \\frac{dJ}{dw_j} = \\frac{dL_{NLL}}{dw_j} = \\frac{dL_{NLL}}{d\\hat{y}} \\frac{d\\hat{y}}{dw_j} = \\frac{dL_{NLL}}{d\\hat{y}} \\frac{d\\hat{y}}{dz} \\frac{dz}{dw_j} = \\frac{dL_{NLL}}{d\\hat{y}} \\frac{d\\sigma{(z)}}{dz} \\frac{dz}{dw_j} $$</p><p>For the first term, the definition $L_{NLL} = y \\log{\\hat{y}} + (1-y)\\log{(1-\\hat{y})}$, giving us the following after some algebra:</p><p>$$ \\frac{dL_{NLL}}{d\\hat{y}} = \\frac{\\hat{y} - y}{\\hat{y}(1-\\hat{y})} .$$</p><p>For the second term, our derivative of the activation function $\\sigma(z)$, we get:</p><p>$$ \\frac{d\\sigma{(z)}}{dz} = \\sigma(z)(1-\\sigma(z)  = \\hat{y}(1-\\hat{y}).$$</p><p>And finally, the last term is simply $\\frac{dz}{dw_j} = x$, except for $\\frac{dz}{dw_0} = 1$.</p>"
            }
          ]
          },

        {
          "id": "project-setup",
          "title": "Project Setup",
          "content": "<pre><code># Clone the repository\ngit clone https://github.com/Uvira007/single-neuron-model\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install dependencies\npip install -r requirements.txt</code></pre><p>Run the Single_Neuron_Model.ipynb notebook from top-to-bottom. Restart the kernel and clear the outputs, if the gradients explode. </p>"
        },
        {
          "id": "model-results",
          "title": "Model Results",
          "content": "<p>The regression model had a Mean Squared Error of 0.049 and the model fit is shown below. The classification model has an accuracy of 91.47%</p><div style='display: flex; justify-content: center; align-items: center; gap: 2rem; flex-wrap: wrap; margin-top: 1.5rem;'><img src='assets/projects/single-neuron-model/Regression-Result.png' alt='Regression Model Result' style='height: 320px; width: 350px; border-radius: 8px;'><img src='assets/projects/single-neuron-model/Classification-Result.png' alt='Classification Model Result' style='height: 320px; width: 350px; border-radius: 8px;'></div>"
        },
        {
          "id": "conclusion",
          "title": "Conclusion",
          "content": "<p>Now although classes are a very powerful tool, they are still not powerful enough to allow our Single Neuron Model to determine good decision boundaries for non linearly separable data. The <a href=\"https://pytorch.org/\" target=\"_blank\">Pytorch library</a> uses all of the afore mentioned concepts to create extremely sophisticated Neural Networks that will allow us to tackle more complex datasets.</p>"
        }
      ]
    },
    {
      "id": "ufo-analysis",
      "title": "Identifying UFO Sightings Patterns Using Text Analysis",
      "description": "Traditional NLP techniques and unsupervised machine learning algorithms are used in this project to analyze the trove of documents released by CIA on UFO sightings to identify hidden patterns.",
      "image": "assets/projects/ufo-analysis/base-image.png",
      "readTime": "12 minute read",
      "tags": ["K-Means", "elbow method", "silhouette coefficient", "NLP", "pdfminer", "cosine similarity", "Vector Space Model", "nltk", "tf-idf"],
      "sections": [
        {
          "id": "project-overview",
          "title": "Project Overview",
          "content": "<p>There has been a recent uptick in the conversations surrounding the notion of \"Unidentified Flying Objects,\" or UFOs, in the media due to Congressional inquiries on the topic. The United States Government Freedom of Information Act (FOIA) allows individual citizens the right to ask for and receive previously unreleased documents possessed by the Government upon request. When these documents are released according to the law, \"Internet detectives\" pour through the documents to find that hidden nugget of information. In the case of this project, we want to see if there are any hidden patterns surrounding the origin of these so-called UFOs that the Government hasn't disclosed before. We will apply our developed system to a series of documents released by the Central Intelligence Agency (CIA) through a FOIA request to better understand the nature of the data.</p>",
          "subsections": [
            {
              "id": "try-it",
              "title": "Try it",
              "content": "<p><a href=\"https://foia-on-ufo-analysis.streamlit.app/\" target=\"_blank\" rel=\"noopener\" style=\"display: inline-block; background: #1f77b4; color: white; padding: 0.6rem 1.2rem; border-radius: 8px; text-decoration: none; font-weight: bold; margin-bottom: 0.5rem;\">▶ Try the live demo</a></p><p>Upload your own FOIA document sets or explore the CIA UFO document collection. Use the Vector Space Model search and K-means clusters to find patterns and similar documents.</p><p><em>Note: The application runs on Streamlit Community Cloud and may take a moment to spin up after inactivity.</em></p>"
            },
            {
              "id": "problem-description",
              "title": "Problem Description",
              "content": "<p>Some of the technical challenges of this project include:</p><ul><li><b>Large Troves of Documents.</b> When released, a trove of documents is published all at once. These can include hundreds of PDFs with thousands of pages. For this particular release of documents from the CIA, there are 712 documents composed of 3493 total pages.</li><li><b>Data That is Dirty.</b> The Government releases documents that are scans of printed materials, with redactions made. This is to ensure that nothing is accidentally revealed that isn't supposed to be, so a physical step is required in Government release procedures. As such, any useful digital representation that may have existed is obliterated. The scans tend not to be sophisticated or of high-quality</li><li><b>Unusual Lexicon.</b> A standard dictionary methodology may not be appropriate for certain document types, especially this dataset. The unusual lexicon surrounding both alien and terrestrial technology will require an unsupervised approach to performing processing.</li><li><b>No Categorization or Labeling.</b> Documents are released without any labeling or categorization of any kind. This leaves it as an exercise for the recipient to pour through the mounds of information to find the needle in the haystack to find the information.</li></ul>"
            },
            {
              "id": "actions",
              "title": "Actions",
              "content": "<p>This tool will be designed generically to accommodate any type of cache of FOIA-released documents from the Government in PDF form. Entire directories of documents will be processed within.</p><p>\nThere will be two primary user classes for the developed software. The first user class is the <i>data scientist</i> who will set up the backend for data processing, including administering the parameters for the model generation steps. The second user class is a <i>citizen data consumer</i> interested in understanding the document set, especially for finding the hidden patterns within the data. In addition, this user class will leverage the clusters that have been exposed.</p>"
            },
            {
              "id": "results",
              "title": "Results",
              "content": "<p>A k-value of 10 is chosen based on the elbow method and silhouette coefficient. The silhouette scores showed low confidence in the cluster assignment and it was reflected in the clusters during subsequent runs. There was variability in the cluster contents.</p><p>\n The query search implementation using Vector Space Model proved useful in finding similar documents for the given search text. One of the advantages of this application is that it is generalized to be used for documents from any domain, making it powerful to do initial analysis for NLP tasks.</p>"
            },
            {
              "id": "growth",
              "title": "Growth/Next Steps",
              "content": "<p>The application can be accessed via <a href=\"https://foia-on-ufo-analysis.streamlit.app/\" target=\"_blank\"><u>FOIA UFO Analysis</u></a>. Currently the application runs on streamlit community cloud with limited resources. The application can be expanded to run at-scale by utilizing a paid cloud services.</p>"
            }
          ]
        },
        {
          "id": "data-overview",
          "title": "Sample Data Overview",
          "content": "<p>A total of 712pdf documents, containing 3493 pages of text related to UFO were converted from the tiff images from the CIA. The document collection consists of a mixture of scans of printed materials with redactions, memos, transcripts and high quality report documents. A sample for low quality redacted document(left) and a high quality report document(right) is shown below</p><div style='display: flex; justify-content: center; align-items: center; gap: 7rem; flex-wrap: wrap; margin-top: 1.5rem;'><img src='assets/projects/ufo-analysis/low-quality-data-image.png' alt='low quality' style='height: 320px; width: 350px; border-radius: 8px;'><img src='assets/projects/ufo-analysis/high-quality-data-image.png' alt='High quality' style='height: 320px; width: 350px; border-radius: 8px;'></div>"
        },
        {
          "id": "system-architecture",
          "title": "System Architecture Overview",
          "content": "<p>The overall architecture will be designed around three subsystems and pipelines, the Backend Data Subsystem (in Blue), the Data Processing Pipeline (in Orange), and the Visualization Subsystem (in Green):</p><div style='display: flex; justify-content: center; align-items: center; gap: 2rem; flex-wrap: wrap; margin-top: 1.5rem;'><img src='assets/projects/ufo-analysis/architecture.PNG' alt='Architecture' style='border-radius: 8px;'></div><p><h3>Backend Data Subsystem</h3>The backend data subsystem will be responsible for opening all of the OCR'd PDF files within the specified directory, cleaning them as appropriate, and building the interim data format for the data processing pipeline.<ul><li>Documents are parsed and imported into memory.</li><li>All words will be compared to a Wordnet, and only words with semantics will be indexed.</li><li>Only documents that ultimately have more than two actual words will be indexed.</li></ul><p>The goal of these conditioning steps is to remove the numerous incorrectly recognized words in some of the more poorly scanned documents. This will result in a more accurate representation of the hidden patterns within the documents. Finally, as a stretch goal, we will have an accessor available so that a user can choose a specific document from a cluster displayed on the rendered visualizations and see the contents of the original (source) document that was used.</p><p><h3>Data Processing Pipeline</h3>The data processing pipeline will be responsible for building all elements required for indexing, querying, and building clusters of our data.<p><ul><li>Build features of the data set by leveraging a count of the words used in the documents.</li><li>Calculate the Sum of Squared distances for a range of clusters, likely from 2 to 20.</li><li>Determine the optimal number of clusters through analysis of the \"elbow method.\"</li><li>Cluster PDFs by fitting the model to the entire data set.</li></ul></p><p>This pipeline will allow the modification of a number of parameters around this clustering technique. A few examples of these include the initial cluster space, often done via random seeding. A second is the number of clusters that the documents will be fit to. Finally, the word frequency for a given cluster as presented to the user. In addition, this data processing pipeline will create all appropriate indexes so that querying can be accomplished through the web-based frontend.</p><p><h3>Visualization Subsystem</h3>The visualization subsystem will be responsible for displaying user interfaces that will allow for both the administering of the data processing pipeline parameters and also the visualization of the results of the analysis. Modifiable parameters in the User Interface will include:<p><ul><li>Cluster Size</li><li>Clustering Initialization Method</li><li>Number of Iterations</li><li>Feature Extraction Method</li></ul></p></p></p></p>"
        },
        {
          "id": "algorithm-overview",
          "title": "Algorithm Overview",
          "content": "<p>Our methodology employs a dual-layered analysis to navigate the document corpus. We first utilize <b>K-means clustering</b>, an unsupervised learning algorithm, to partition the documents into thematic groups and reveal inherent structural patterns. To complement this, we implement a <b>Vector Space Model</b> that transforms textual data into high-dimensional numerical vectors. By applying <b>TF-IDF weighting</b> and <b>cosine similarity</b>, the system enables precise search capabilities by measuring the angular distance between query and document vectors, ensuring that the most contextually relevant results are retrieved.</p><ul><li><b>K-means Clustering:</b> An iterative process that minimizes the distance between data points and a central \"centroid\" to form compact, separate groups of similar documents.</li><li><b>Vector Space Model (VSM):</b> A mathematical model where each document is treated as a vector in a multi-dimensional space, with each unique word representing a dimension.</li><li><b>TF-IDF & Cosine Similarity:</b> TF-IDF weights words based on their importance to a specific document versus the whole collection, while cosine similarity measures the angle between vectors to determine how \"close\" two documents are in meaning.</li>"
        },
        {
          "id": "project-setup",
          "title": "Project Setup",
          "content": "<p>The application is currently hosted on the Streamlit Community Cloud for immediate use. You may conduct your own analysis by uploading document sets directly to the <a href=\"https://foia-on-ufo-analysis.streamlit.app/\" target=\"_blank\"><u>FOIA UFO Analysis Portal</u></a>. Alternatively, for those who prefer a local environment, the following instructions outline the setup process for private deployment.</p><pre><code># Clone and setup\ngit clone https://github.com/Uvira007/foia-on-ufo-analysis\npip install -r requirements.txt\n\n# Ensure to run the command from the root of the project\nstreamlit run src/app/Welcome.py </code></pre>"
        },
          {
          "id": "model-results",
          "title": "Model Results",
          "content": "<p>The plot of elbow method to determine the optimal k value and the word cloud for one of the clusters formed is shown below.</p><div style='display: flex; justify-content: center; align-items: center; gap: 5rem; flex-wrap: wrap; margin-top: 1.5rem;'><img src='assets/projects/ufo-analysis/elbow-method-optimal-k.png' alt='elbow method' style='height: 250px; width: 450px; border-radius: 8px;'><img src='assets/projects/ufo-analysis/word-cloud.png' alt='Word Cloud' style='height: 250px; width: 450px; border-radius: 8px;'></div>"
        },
        {
          "id": "conclusion",
          "title": "Conclusion",
          "content": "<p>This project applied traditional NLP and unsupervised learning to a real-world document release: 712 CIA FOIA documents (3,493 pages) on UFO-related material. Using <b>K-means clustering</b> with the <b>elbow method</b> and <b>silhouette coefficient</b>, we chose k=10 clusters; silhouette scores indicated moderate confidence and some variability in cluster composition across runs. The <b>Vector Space Model</b> with <b>TF-IDF</b> and <b>cosine similarity</b> proved effective for query-based search and finding semantically similar documents. The system is generalized for any FOIA-style document corpus, making it suitable for initial NLP exploration. The application is hosted on Streamlit Community Cloud for public use and can be scaled with paid cloud resources for larger deployments.</p>"
        }
      ]
    },
    {
      "id": "anime-face-generator",
      "title": "Anime Face Generator with DCGAN",
      "description": "A Deep Convolutional Generative Adversarial Network (DCGAN) that generates unique anime character faces, with a full-stack deployment: React frontend, FastAPI backend, and hosting on Hugging Face Spaces, GitHub Pages, and Render.",
      "image": "assets/projects/anime-face-generator/main-image.gif",
      "readTime": "25 minute read",
      "tags": ["DCGAN", "PyTorch", "Generative AI", "FastAPI", "React", "ONNX", "Hugging Face", "GitHub Pages", "Render"],
      "sections": [
        {
          "id": "project-overview",
          "title": "Project Overview",
          "content": "<p>This project implements a <strong>Deep Convolutional Generative Adversarial Network (DCGAN)</strong> to generate unique anime character faces. The model is trained on the Anime Face Dataset (~63,000 images, CC0 license) and deployed as a full-stack application with multiple hosting options.</p>",
          "subsections": [
            {
              "id": "try-it",
              "title": "Try it",
              "content": "<p><a href=\"https://uvira007.github.io/anime-face-generator/\" target=\"_blank\" rel=\"noopener\" style=\"display: inline-block; background: #1f77b4; color: white; padding: 0.6rem 1.2rem; border-radius: 8px; text-decoration: none; font-weight: bold; margin-bottom: 0.5rem;\">▶ Try the live demo</a></p><p>Generate anime faces and try latent-space interpolation (morph between two faces). Also available via <a href=\"https://uvira007-anime-face-generator.hf.space\" target=\"_blank\" rel=\"noopener\">Hugging Face Space</a>.</p><p><em>Note: The app may take a moment to spin up when the backend (e.g. Render) is cold after inactivity.</em></p>"
            },
            {
              "id": "problem",
              "title": "Problem",
              "content": "<p>Generating coherent, diverse anime-style faces from scratch requires learning a high-dimensional distribution of facial features (eyes, hair, expressions) from data. The goal was to build an end-to-end system: train a generative model, export it for efficient inference, and serve it through a web app that hiring managers and peers can try without running code.</p>"
            },
            {
              "id": "actions",
              "title": "Actions",
              "content": "<p>I implemented a DCGAN (Generator + Discriminator) in PyTorch following the <a href=\"https://arxiv.org/abs/1511.06434\" target=\"_blank\">Radford et al. (2015)</a> architecture guidelines; trained on the Kaggle Anime Face Dataset with proper preprocessing (resize, normalize); exported the generator to ONNX for fast inference; built a FastAPI backend with <code>/generate</code> and <code>/interpolate</code> endpoints; created a React frontend with Tailwind CSS and Framer Motion; and deployed the app via <strong>Hugging Face Spaces</strong> (Docker), <strong>GitHub Pages</strong> (frontend) with <strong>Render</strong> (API and model serving).</p>"
            },
            {
              "id": "results",
              "title": "Result",
              "content": "<p>A working anime face generator that produces 64×64 RGB images from random latent vectors, supports latent-space interpolation (smooth morphing between two faces), and is accessible via a public demo. The project demonstrates full-cycle ML: data pipeline, model design, training, export, API design, and deployment.</p>"
            }
          ]
        },
        {
          "id": "sample-data-overview",
          "title": "Sample Data Overview",
          "content": "<p>The model is trained on the <a href=\"https://www.kaggle.com/datasets/splcher/animefacedataset\" target=\"_blank\">Anime Face Dataset</a> from Kaggle.</p><ul><li><b>Total images:</b> ~63,000 anime character face images</li><li><b>Format:</b> JPG/PNG; original resolutions vary (typically 64×64 to 512×512)</li><li><b>License:</b> CC0 (Public Domain)</li><li><b>Preprocessing:</b> Images are resized to 64×64 and normalized to the range [-1, 1] for training</li></ul><p>The dataset is downloaded via the Kaggle API or manually from the link above and extracted into a single directory (e.g. <code>data/anime_faces/</code>) for the training script.</p>"
        },
        {
          "id": "network-architecture",
          "title": "Network Architecture",
          "content": "<p>The DCGAN follows the architecture guidelines from Radford et al.: strided convolutions (no pooling), batch normalization in both networks (except generator output and discriminator input), no fully connected hidden layers, ReLU in the generator (except output), and LeakyReLU in the discriminator.</p>",
          "subsections": [
            {
              "id": "generator",
              "title": "Generator",
              "content": "<p>The generator maps a 100-dimensional latent vector <code>z</code> to a 64×64 RGB image. It uses transposed convolutions to upsample from 4×4 to 64×64.</p><pre><code>Input: (batch, 100, 1, 1) - Random latent vector\n\nLayer 1: ConvTranspose2d(100, 512, 4, 1, 0) → BatchNorm → ReLU  → (batch, 512, 4, 4)\nLayer 2: ConvTranspose2d(512, 256, 4, 2, 1) → BatchNorm → ReLU  → (batch, 256, 8, 8)\nLayer 3: ConvTranspose2d(256, 128, 4, 2, 1) → BatchNorm → ReLU  → (batch, 128, 16, 16)\nLayer 4: ConvTranspose2d(128, 64, 4, 2, 1)  → BatchNorm → ReLU  → (batch, 64, 32, 32)\nLayer 5: ConvTranspose2d(64, 3, 4, 2, 1)    → Tanh              → (batch, 3, 64, 64)\n\nOutput: (batch, 3, 64, 64) - RGB image in [-1, 1]</code></pre><p>Weights are initialized with mean 0 and std 0.02 (Conv); BatchNorm with mean 1 and std 0.02.</p>"
            },
            {
              "id": "discriminator",
              "title": "Discriminator",
              "content": "<p>The discriminator is a binary classifier: real vs. generated. It uses strided convolutions to downsample from 64×64 to a single scalar probability.</p><pre><code>Input: (batch, 3, 64, 64) - RGB image\n\nLayer 1: Conv2d(3, 64, 4, 2, 1)    → LeakyReLU(0.2)  → (batch, 64, 32, 32)\nLayer 2: Conv2d(64, 128, 4, 2, 1)  → BN → LeakyReLU  → (batch, 128, 16, 16)\nLayer 3: Conv2d(128, 256, 4, 2, 1) → BN → LeakyReLU  → (batch, 256, 8, 8)\nLayer 4: Conv2d(256, 512, 4, 2, 1) → BN → LeakyReLU  → (batch, 512, 4, 4)\nLayer 5: Conv2d(512, 1, 4, 1, 0)   → Sigmoid         → (batch, 1, 1, 1)\n\nOutput: Probability that input is real [0, 1]</code></pre>"
            }
          ]
        },
        {
          "id": "dcgan-model-overview",
          "title": "DCGAN Model Overview",
          "content": "<p>A DCGAN trains two networks simultaneously: the <b>Generator (G)</b> creates fake images from random noise; the <b>Discriminator (D)</b> tries to distinguish real images from fakes. Through adversarial training, G learns to produce increasingly realistic images while D gets better at detecting fakes. Training uses binary cross-entropy loss: D is trained to maximize log D(x) + log(1 − D(G(z))), and G is trained to maximize log D(G(z)). Key hyperparameters: latent dimension <code>nz=100</code>, generator/discriminator feature maps <code>ngf=ndf=64</code>, batch size 128, learning rate 0.0002, Adam with beta1=0.5. After training, only the generator is exported to ONNX for deployment.</p>"
        },
        {
          "id": "system-architecture",
          "title": "System Architecture",
          "content": "<p>The application is split into a frontend (UI) and a backend (API + model inference).</p>",
          "subsections": [
            {
              "id": "frontend",
              "title": "Front end",
              "content": "<p><b>Stack:</b> React 18, Vite, Tailwind CSS, Framer Motion.</p><p>The UI lets users (1) generate a grid of anime faces (with optional seed for reproducibility) and (2) interpolate between two faces by choosing two seeds and a step count. It calls the backend REST API (<code>POST /generate</code>, <code>POST /interpolate</code>), displays returned base64 images, and supports download. The API base URL is configurable via environment variable (e.g. for GitHub Pages + Render, the frontend points to the Render API URL).</p>"
            },
            {
              "id": "backend",
              "title": "Backend",
              "content": "<p><b>Stack:</b> FastAPI, Uvicorn, ONNX Runtime.</p><p>The backend loads the ONNX-exported generator at startup and exposes: <code>GET /</code> (API info), <code>GET /health</code> (model loaded or not), <code>POST /generate</code> (num_images, seed, format=base64|grid), <code>POST /interpolate</code> (seed1, seed2, num_steps), and <code>GET /random?seed=</code> (returns a single PNG). CORS is configured so the frontend (e.g. GitHub Pages or localhost) can call the API. The model file is provided at deployment time (e.g. from repo, GitHub Releases, or Hugging Face).</p>"
            }
          ]
        },
        {
          "id": "project-setup",
          "title": "Project Setup",
          "content": "<p>Clone the repo and run the stack locally with Docker, or run frontend and API separately.</p><pre><code># Clone the repository\ngit clone https://github.com/yourusername/anime-face-generator.git\ncd anime-face-generator\n\n# Option 1: Run with Docker (recommended)\n# Place trained generator.onnx in api/models/\nmkdir -p api/models\ncp /path/to/generator.onnx api/models/\ndocker compose up -d\n# Open http://localhost:3000\n\n# Option 2: Run API and frontend separately\n# Terminal 1 - API\ncd api && pip install -r requirements.txt\nuvicorn main:app --reload --host 0.0.0.0 --port 8000\n\n# Terminal 2 - Frontend\ncd frontend && npm install && npm run dev\n# Set VITE_API_URL=http://localhost:8000 if needed</code></pre><p>Training and ONNX export are in the <code>model/</code> directory; see the Training section for dataset and training instructions.</p>"
        },
        {
          "id": "training",
          "title": "Training",
          "content": "<p>Training is done with PyTorch using the scripts in <code>model/</code> (no notebooks required).</p><ul><li><b>Dataset:</b> Download via Kaggle API: <code>kaggle datasets download -d splcher/animefacedataset -p ./data --unzip</code>, or download manually and extract to <code>./data/anime_faces/</code>.</li><li><b>Command:</b> <code>cd model && python train.py --data_dir ./data/anime_faces --epochs 100</code> (optional: <code>--batch_size 128 --lr 0.0002 --device cuda</code>).</li><li><b>Outputs:</b> Checkpoints and sample images are saved under <code>checkpoints/</code> and <code>outputs/</code>. Typical training time: ~30–60 min for 100 epochs on a modern GPU.</li><li><b>Export:</b> <code>python export_onnx.py --checkpoint ./checkpoints/generator_epoch_final.pth --output ./exports/generator.onnx</code>. Copy the ONNX file to <code>api/models/generator.onnx</code> for the API.</li></ul><p>Healthy training: D(x) around 0.7–0.9, D(G(z)) around 0.3–0.5; sample images improve over epochs. If the discriminator dominates (D(G(z))→0), try lowering learning rate or using label smoothing.</p><div style='display: flex; justify-content: center; align-items: center; flex-wrap: wrap; margin-top: 1.5rem;'><img src='assets/projects/anime-face-generator/training-progression.gif' alt='Training progression' style='max-width: 480px; width: 100%; height: auto; border-radius: 8px;'></div>"
        },
        {
          "id": "model-hosting-usage",
          "title": "Model Hosting / Usage",
          "content": "<p>The project supports multiple deployment options so the demo can be shared without cost.</p>",
          "subsections": [
            {
              "id": "hugging-face",
              "title": "Hugging Face Spaces",
              "content": "<p><strong>Hugging Face Spaces</strong> hosts both the app and the model in one place, with no time limits and no credit card. Create a new Space with SDK <b>Docker</b>, then add your Dockerfile, <code>app.py</code> (Gradio or custom app that loads the ONNX model and exposes an interface), <code>requirements.txt</code>, and the ONNX model (e.g. under <code>api/models/</code>). Push to the Space repo; Hugging Face builds and runs the container. The app is available at <code>https://uvira007-anime-face-generator.hf.space</code>. Ideal for ML demos and portfolios.</p>"
            },
            {
              "id": "github-pages-render",
              "title": "GitHub Pages + Render",
              "content": "<p><strong>Frontend (GitHub Pages):</strong> The React app is built and deployed via GitHub Actions to GitHub Pages, so the UI is served at <code>https://uvira007.github.io/anime-face-generator/</code>. Configure the workflow with the correct base path and ensure the frontend uses the API URL (e.g. from a secret or env).</p><p><strong>Backend (Render):</strong> The FastAPI app is deployed as a Web Service on Render. Set the root directory to <code>api</code>, add environment variables (<code>MODEL_PATH</code>, <code>LATENT_DIM</code>, <code>CORS_ORIGINS</code>), and ensure the ONNX model is available at startup (e.g. from GitHub Releases or included in the repo). Render’s free tier may spin down after inactivity; the first request after spin-down can take 30–60 seconds. Once the API URL is set, the GitHub Pages frontend calls it for generation and interpolation.</p>"
            }
          ]
        },
        {
          "id": "results",
          "title": "Results",
          "content": "<p>After training (e.g. 50–100 epochs), the generator produces diverse anime-style faces. Below are sample outputs and an example of latent-space interpolation (morphing between two faces).</p><div style='display: flex; justify-content: center; align-items: center; gap: 2rem; flex-wrap: wrap; margin-top: 1.5rem;'><img src='assets/projects/anime-face-generator/generated-anime-face.png' alt='Generated anime faces' style='max-width: 100%; height: auto; border-radius: 8px;'></div><p style='margin-top: 1rem; text-align: center;'><i>Sample generated anime faces after training.</i></p><div style='display: flex; justify-content: center; align-items: center; flex-wrap: wrap; margin-top: 1.5rem;'><img src='assets/projects/anime-face-generator/interpolation-sequence.png' alt='Latent interpolation' style='max-width: 100%; height: auto; border-radius: 8px;'></div><p style='margin-top: 1rem; text-align: center;'><i>Smooth transition between two faces via latent-space interpolation.</i></p>"
        },
        {
          "id": "conclusion",
          "title": "Conclusion",
          "content": "<p>This project demonstrates the full pipeline of a generative ML application: dataset preparation, DCGAN design and training in PyTorch, ONNX export for inference, a FastAPI backend for model serving, and a React frontend for interaction. By documenting deployment on <strong>Hugging Face Spaces</strong> and <strong>GitHub Pages + Render</strong>, the project is accessible to hiring managers and peers without local setup. It highlights both deep learning (GANs, architecture choices, training dynamics) and engineering (APIs, Docker, and cloud deployment).</p>"
        }
      ]
    },
    {
      "id": "stock-predictor",
      "title": "Stock Predictor with Multivariate GRU",
      "description": "End-to-end stock price predictor using a GRU built from scratch (no nn.GRU), multivariate OHLCV inputs from Yahoo Finance, and a single model for all tickers. FastAPI backend on Render, Streamlit UI on Streamlit Community Cloud, with automated retrain and model updates via GitHub Actions and GitHub API.",
      "image": "assets/projects/stock-predictor/stock-prediction.jpg",
      "readTime": "24 minute read",
      "tags": ["GRU", "PyTorch", "Time Series", "Yahoo Finance", "FastAPI", "Streamlit", "ONNX", "Render", "GitHub API", "GitHub Actions"],
      "sections": [
        {
          "id": "project-overview",
          "title": "Project Overview",
          "content": "<p>This project implements an end-to-end <strong>stock price predictor</strong> using a <strong>GRU built from scratch</strong> (no <code>nn.GRU</code>), multivariate Open/High/Low/Close/Volume (OHLCV) inputs from <strong>Yahoo Finance</strong>, and a single model for all tickers via ticker embedding. The backend runs on <strong>Render</strong>, the Streamlit UI on <strong>Streamlit Community Cloud</strong>, and the workflow is automated through <strong>GitHub Actions</strong> and <strong>GitHub API</strong> to update model parameters when the model is fine-tuned or retrained.</p>",
          "subsections": [
            {
              "id": "live-demo",
              "title": "Try it",
              "content": "<p><a href=\"https://gru-stock-predictor.streamlit.app/\" target=\"_blank\" rel=\"noopener\" style=\"display: inline-block; background: #1f77b4; color: white; padding: 0.6rem 1.2rem; border-radius: 8px; text-decoration: none; font-weight: bold; margin-bottom: 0.5rem;\">▶ Try the live demo</a></p><p>Select a ticker (e.g. MSFT, AAPL), choose how many days ahead to predict (1–21), and run a prediction.</p><p><em>Note: The application may take several minutes to spin up when both the backend (Render) and frontend (Streamlit) are cold; the first request after inactivity can take 30–60 seconds or more.</em></p>"
            },
            {
              "id": "actions",
              "title": "Features",
              "content": "<p><ul><li><b>GRU from scratch:</b> Custom <code>GRUCell</code> and multi-layer <code>GRU</code> in <code>src/model/gru.py</code></li><li><b>Multivariate:</b> Open, High, Low, Close, Volume (normalized; log-scale volume)</li><li><b>Single model for all tickers:</b> Ticker embedding; one training run, one checkpoint</li><li><b>Next N days:</b> Predict next 1–21 days closing price (configurable)</li><li><b>Isolated backend:</b> FastAPI API; Streamlit UI calls the API</li><li><b>Retrain option:</b> Optional retrain with latest data; workflow automated via GitHub Actions and GitHub API</li><li><b>Export:</b> PyTorch (<code>.pt</code>, <code>config.json</code>, <code>normalize_stats.json</code>) and ONNX</li></ul></p>"
            }
          ]
        },
        {
          "id": "data-overview",
          "title": "Data Overview",
          "content": "<p>Data is fetched from <strong>Yahoo Finance</strong> using <a href=\"https://pypi.org/project/yfinance/\" target=\"_blank\">yfinance</a> (free, no API key). Default training uses ~5 years of daily OHLCV for tickers such as AAPL, MSFT, GOOGL, AMZN, META, NVDA, TSLA, JPM, V, JNJ. Features are normalized per ticker; volume is log-scaled. Sequences are built with a configurable lookback (<code>seq_len=60</code> by default) and target the next N closing prices (<code>predict_days=1–21</code>). For research and education; not for live trading.</p><div style='display: flex; justify-content: center; align-items: center; flex-wrap: wrap; margin-top: 1.5rem;'><img src='assets/projects/stock-predictor/data-sample.png' alt='Sample OHLCV data' style='max-width: 600px; width: 100%; height: auto; border-radius: 8px;'></div><p style='margin-top: 1rem; text-align: center;'><i>Sample OHLCV time series (one ticker) used as input to the model.</i></p>"
        },
        {
          "id": "network-architecture",
          "title": "Model Architecture",
          "content": "<p>The <strong>StockGRUModel</strong> is a hybrid architecture that combines <b>categorical</b> (ticker index via embedding) and <b>sequential</b> (OHLCV via GRU) inputs to predict future closing prices. The diagram below shows the high-level prediction flow: the <b>Ticker Index</b> and <b>OHLCV Sequence (T steps)</b> are processed in parallel—the ticker through an <b>Embedding</b> layer and the sequence through a <b>GRU module (stacked layers)</b>. The resulting <b>Embedding Vector</b> and <b>Final Hidden State</b> are concatenated and passed through a <b>Prediction Head (MLP)</b> to produce <b>Next-N-Days Predictions</b>.</p><div style='display: flex; justify-content: center; align-items: center; flex-wrap: wrap; margin-top: 1.5rem;'><img src='assets/projects/stock-predictor/prediction-flow.png' alt='Stock predictor high-level prediction flow' style='max-width: 600px; width: 100%; height: auto; border-radius: 8px;'></div><p style='margin-top: 1rem; text-align: center;'><i>Prediction flow: Ticker Index + OHLCV Sequence → Ticker Embedding + GRU → Concatenated Vector → Prediction Head (MLP) → Next-N-days predictions.</i></p><p>The full network architecture diagram below details the model components: <b>Ticker Embedding (B, 16)</b>, two <b>GRULayers</b> (hidden_size=64) with <b>Dropout (0.2)</b> between them, a <b>GRUCell</b> view (update gate <code>z</code>, reset gate <code>r</code>, candidate <code>h̃</code>, new state <code>h'</code>), <b>Concatenate (B, 80)</b> of final hidden state and embedding, and the <b>Prediction Head (MLP)</b>: Linear (80→64), ReLU, Dropout, Linear (64→21). Training uses backpropagation to update the GRU weights (Wz, Wr, Wh) and the MLP parameters.</p><div style='display: flex; justify-content: center; align-items: center; flex-wrap: wrap; margin-top: 1.5rem;'><img src='assets/projects/stock-predictor/GRU-network-architecture.png' alt='StockGRUModel full GRU network architecture' style='max-width: 600px; width: 100%; height: auto; border-radius: 8px;'></div><p style='margin-top: 1rem; text-align: center;'><i>Full StockGRUModel: inputs, GRU layers, GRUCell detail, concatenation, and MLP head with backpropagation.</i></p>"
        },
        {
          "id": "algorithm-overview",
          "title": "GRU from Scratch",
          "content": "<p>The custom GRU uses a single <b>GRUCell</b>: update gate <code>z = σ(W_z @ [x, h])</code>, reset gate <code>r = σ(W_r @ [x, h])</code>, candidate <code>h̃ = tanh(W @ [x, r*h])</code>, and new state <code>h' = (1−z)*h + z*h̃</code>. Multiple <b>GRULayer</b>s are stacked (input_size → hidden_size per layer, with dropout between layers). The full model: <b>Embedding(ticker_id)</b> + <b>GRU(sequence)</b> → last hidden state concatenated with embedding → <b>FC</b> → (B, predict_days). Default: hidden_size=64, num_gru_layers=2, embedding_dim=16, seq_len=60, predict_days=21.</p>"
        },
        {
          "id": "project-setup",
          "title": "Project Setup",
          "content": "<p>Clone the repo and install dependencies. Train once, then run the API and Streamlit app.</p><pre><code># Clone the repository\ngit clone https://github.com/Uvira007/stock-predictor/\ncd stock-predictor\n\n# Create virtual environment\npython -m venv .venv\n.venv\\Scripts\\activate   # Windows: .venv\\Scripts\\activate\npip install -r requirements.txt\n\n# 1. Train the model (downloads ~5y OHLCV, saves to models/)\npython scripts/train_model.py\n\n# 2. Start the backend API (http://127.0.0.1:8000)\npython scripts/run_api.py\n\n# 3. In a second terminal: Streamlit UI\nstreamlit run app/streamlit_app.py</code></pre><p>API endpoints: <code>GET /tickers</code>, <code>POST /predict</code> (body: <code>{\"ticker\": \"AAPL\", \"n_days\": 5}</code>), <code>POST /retrain</code>, <code>GET /model/status</code>. Export to PyTorch and ONNX: <code>python scripts/export_model.py</code>.</p>"
        },
        {
          "id": "model-results",
          "title": "Model Results",
          "content": "<p>Example prediction: <strong>MSFT — next 21 days</strong> from a given start date. The model outputs normalized closing prices, which are inverse-transformed using per-ticker stats for display. The chart shows the predicted close (USD) over the next 21 days.</p><div style='display: flex; justify-content: center; align-items: center; flex-wrap: wrap; margin-top: 1.5rem;'><img src='assets/projects/stock-predictor/result.png' alt='MSFT 21-day prediction' style='max-width: 600px; width: 100%; height: auto; border-radius: 8px;'></div><p style='margin-top: 1rem; text-align: center;'><i>Sample prediction: MSFT next 21 days closing price (multivariate GRU, Yahoo Finance data).</i></p>"
        },
        {
          "id": "conclusion",
          "title": "Conclusion",
          "content": "<p>This project demonstrates a full pipeline: GRU implemented from scratch, multivariate time-series data (Yahoo Finance), a single multi-ticker model with embeddings, FastAPI backend and Streamlit frontend, and deployment on <strong>Render</strong> and <strong>Streamlit Community Cloud</strong>. The workflow is automated with <strong>GitHub Actions</strong> and <strong>GitHub API</strong> to update model parameters when the model is retrained or fine-tuned. Export to PyTorch and ONNX allows reuse of the model outside the repo. For code and setup, see <a href=\"https://github.com/Uvira007/stock-predictor/\" target=\"_blank\">GitHub — Uvira007/stock-predictor</a>.</p>"
        }
      ]
    },
    {
      "id": "customer-segmentation",
      "title": "Customer Segmentation Using K-Means Clustering",
      "description": "In this project we use unsupervised learning to segment customers based on purchasing behavior, enabling targeted marketing strategies.",
      "image": "assets/projects/segmentation.jpg",
      "readTime": "20 minute read",
      "tags": ["Machine Learning", "Clustering", "Data Science", "Python", "K-Means"],
      "sections": [
        {
          "id": "project-overview",
          "title": "Project Overview",
          "content": "<p>This project applies K-Means clustering to identify distinct customer segments from transaction data, enabling personalized marketing.</p>",
          "subsections": [
            {
              "id": "context",
              "title": "Context",
              "content": "<p>One-size-fits-all marketing is inefficient. Understanding different customer segments allows for more effective targeting and messaging.</p>"
            },
            {
              "id": "actions",
              "title": "Actions",
              "content": "<p>Our analysis included:</p><ul><li>RFM (Recency, Frequency, Monetary) feature engineering</li><li>Elbow method and silhouette analysis for optimal K</li><li>Cluster profiling and interpretation</li><li>Actionable segment-specific strategies</li></ul>"
            },
            {
              "id": "results",
              "title": "Results",
              "content": "<p>Identified <strong>5 distinct customer segments</strong> with clear behavioral profiles, enabling a 23% improvement in campaign response rates.</p>"
            },
            {
              "id": "growth",
              "title": "Growth/Next Steps",
              "content": "<p>Implement real-time segment assignment for new customers and monitor segment migration over time.</p>"
            }
          ]
        },
        {
          "id": "data-overview",
          "title": "Data Overview",
          "content": "<p>Transaction history from 10,000 customers over 2 years, including purchase dates, amounts, and product categories.</p>"
        },
        {
          "id": "algorithm-overview",
          "title": "K-Means Algorithm Overview",
          "content": "<p>K-Means partitions data into K clusters by minimizing within-cluster variance. We used standardized features and multiple random initializations for stability.</p>"
        },
        {
          "id": "project-setup",
          "title": "Project Setup",
          "content": "<pre><code>pip install -r requirements.txt\npython segmentation_analysis.py</code></pre>"
        },
        {
          "id": "demo",
          "title": "Demo",
          "content": "<p>Interactive visualization of customer segments with drill-down capabilities.</p>"
        },
        {
          "id": "conclusion",
          "title": "Conclusion",
          "content": "<p>Customer segmentation provides actionable insights for marketing personalization and resource allocation.</p>"
        }
      ]
    },
    {
      "id": "association-rules",
      "title": "Market Basket Analysis Using Association Rules",
      "description": "In this project we discover which products are frequently purchased together, enabling strategic product placement and cross-selling opportunities.",
      "image": "assets/projects/market-basket.jpg",
      "readTime": "18 minute read",
      "tags": ["Data Mining", "Association Rules", "Data Science", "Python", "Apriori"],
      "sections": [
        {
          "id": "project-overview",
          "title": "Project Overview",
          "content": "<p>This project uses the Apriori algorithm to discover product associations in retail transaction data.</p>",
          "subsections": [
            {
              "id": "context",
              "title": "Context",
              "content": "<p>Understanding which products are bought together enables better store layout, promotions, and recommendation systems.</p>"
            },
            {
              "id": "actions",
              "title": "Actions",
              "content": "<p>We performed:</p><ul><li>Transaction data preprocessing</li><li>Apriori algorithm for frequent itemset mining</li><li>Association rule generation with support/confidence thresholds</li><li>Network visualization of product relationships</li></ul>"
            },
            {
              "id": "results",
              "title": "Results",
              "content": "<p>Discovered <strong>47 high-confidence rules</strong> with lift > 3, leading to store layout optimizations that increased basket size by 8%.</p>"
            },
            {
              "id": "growth",
              "title": "Growth/Next Steps",
              "content": "<p>Extend to online recommendation engine and seasonal pattern analysis.</p>"
            }
          ]
        },
        {
          "id": "data-overview",
          "title": "Data Overview",
          "content": "<p>Transaction data from 50,000 shopping baskets containing 200 unique products.</p>"
        },
        {
          "id": "algorithm-overview",
          "title": "Apriori Algorithm Overview",
          "content": "<p>Apriori efficiently finds frequent itemsets by pruning candidates that contain infrequent subsets. Rules are then generated with configurable support and confidence thresholds.</p>"
        },
        {
          "id": "project-setup",
          "title": "Project Setup",
          "content": "<pre><code>pip install mlxtend\npython basket_analysis.py</code></pre>"
        },
        {
          "id": "demo",
          "title": "Demo",
          "content": "<p>Interactive network graph showing product relationships and association strengths.</p>"
        },
        {
          "id": "conclusion",
          "title": "Conclusion",
          "content": "<p>Association rule mining reveals hidden patterns in transaction data that drive practical business decisions.</p>"
        }
      ]
    }
  ]
}
